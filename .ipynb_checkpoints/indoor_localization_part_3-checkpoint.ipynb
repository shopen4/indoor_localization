{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Python >= 3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3,5)\n",
    "\n",
    "# Scikit-Learn >= 0.20 is required\n",
    "import sklearn \n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "# common imports\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import os \n",
    "\n",
    "# to plot pretty figures\n",
    "%matplotlib inline \n",
    "import matplotlib as mpl \n",
    "import matplotlib.pyplot as plt \n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "DATASET_PATH = os.path.join(PROJECT_ROOT_DIR, \"wifi_dataset\")\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\")\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to csv files\n",
    "path_to_train_csvs = os.path.join(DATASET_PATH, \"train\")\n",
    "path_to_val_csvs = os.path.join(DATASET_PATH, \"val\")\n",
    "path_to_test_csvs = os.path.join(DATASET_PATH, \"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3. <font color=green>Predicting of a user's coordinates using feed forward neural networks</font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1. Copy and paste your previous implementations of <font color=blue>build_feats</font>, <font color=blue>build_feats_targets</font>, and <font color=blue>euclidean_distance</font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_feats(path_to_csvs):\n",
    "    my_data = genfromtxt(f'{path_to_csvs}/1.csv', delimiter=',')\n",
    "    \n",
    "    my_data2 = genfromtxt(f'{path_to_csvs}/2.csv', delimiter=',')\n",
    "    \n",
    "    feats = np.concatenate((my_data, my_data2), axis=0)\n",
    "\n",
    "    \n",
    "    files = os.path.join(path_to_csvs, \"*.csv\")\n",
    "\n",
    "    files = glob.glob(files)\n",
    "    \n",
    "    files.remove(f'{path_to_csvs}/1.csv')\n",
    "    files.remove(f'{path_to_csvs}/2.csv')\n",
    "    \n",
    "    for file in files:\n",
    "        my_data = genfromtxt(file, delimiter=',')\n",
    "        feats = np.concatenate((feats, my_data), axis=0)\n",
    "    \n",
    "    return feats\n",
    "\n",
    "def build_feats_targets(path_to_csvs):\n",
    "    my_data = genfromtxt(f'{path_to_csvs}/1.csv', delimiter=',')\n",
    "    \n",
    "    my_data2 = genfromtxt(f'{path_to_csvs}/2.csv', delimiter=',')\n",
    "    \n",
    "    data_set = np.concatenate((my_data, my_data2), axis=0)\n",
    "    \n",
    "    files = os.path.join(path_to_csvs, \"*.csv\")\n",
    "\n",
    "    files = glob.glob(files)\n",
    "    \n",
    "    files.remove(f'{path_to_csvs}/1.csv')\n",
    "    files.remove(f'{path_to_csvs}/2.csv')\n",
    "    \n",
    "    for file in files:\n",
    "        my_data = genfromtxt(file, delimiter=',')\n",
    "        data_set = np.concatenate((data_set, my_data), axis=0)\n",
    "\n",
    "    feats = data_set[:, :-3]\n",
    "    targets = data_set[:, -3:]\n",
    "    \n",
    "    \n",
    "    \n",
    "    return feats, targets\n",
    "\n",
    "def mean_error_dist(targets, preds):\n",
    "    sum = 0\n",
    "    for i in range(len(targets)):\n",
    "        sum += np.linalg.norm(targets[i] - preds[i])\n",
    "    dist = sum / len(targets)\n",
    "    return dist\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=red>NOTE !</font> \n",
    "Before we feed the data to a neural network, we first need to normalize it and substract the mean for a better convergence. \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats_train, targets_train = build_feats_targets(path_to_train_csvs)\n",
    "feats_train = (feats_train.astype('float32') + 100.0) / 100.0\n",
    "mean = np.mean(feats_train)\n",
    "feats_train_norm = (feats_train - mean)\n",
    "\n",
    "# verify dimensions of the returned feature matrix and a target matrix\n",
    "assert(feats_train.shape == (6049,220))\n",
    "assert(targets_train.shape == (6049,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats_val, targets_val = build_feats_targets(path_to_val_csvs)\n",
    "feats_val = (feats_val.astype('float32') + 100.0) / 100.0\n",
    "feats_val_norm = (feats_val - mean)\n",
    "\n",
    "# verify dimensions of the returned feature matrix and a target matrix\n",
    "assert(feats_val.shape == (1976,220))\n",
    "assert(targets_val.shape == (1976,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats_test = build_feats(path_to_test_csvs)\n",
    "feats_test = (feats_test.astype('float32') + 100.0) / 100.0\n",
    "feats_test_norm = (feats_test - mean)\n",
    "\n",
    "# verify dimensions of the returned feature matrix\n",
    "assert(feats_test.shape == (2601,220))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2. Using feed forward neural networks.\n",
    "For this part, we provide you with a simple feed forward neural network (or a multi layer perceptron). The code is is given below.  You can tune the parameters, extend the network and even modify the model as you see fit in order to find the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_schedule(epoch):\n",
    "    \"\"\"Learning Rate Schedule\n",
    "        # Arguments\n",
    "            epoch (int): The number of epochs\n",
    "\n",
    "        # Returns\n",
    "            lr (float32): learning rate\n",
    "    \"\"\"\n",
    "    lr = 1e-3\n",
    "    if epoch > 50:\n",
    "        lr = 1e-4\n",
    "        \n",
    "    print('Learning rate: ', lr)\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "\n",
    "lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),\n",
    "                               cooldown=0,\n",
    "                               patience=5,\n",
    "                               min_lr=0.5e-6)\n",
    "\n",
    "callbacks = [lr_reducer, lr_scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network parameters\n",
    "input_size = feats_train.shape[1]\n",
    "epochs = 100\n",
    "batch_size = 8\n",
    "hidden_units = 128\n",
    "dropout = 0.1\n",
    "\n",
    "# model is a 3-layer MLP with ReLU and dropout after each layer\n",
    "model = Sequential()\n",
    "model.add(Dense(hidden_units, input_dim=input_size))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(dropout))\n",
    "model.add(Dense(hidden_units))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(dropout))\n",
    "model.add(Dense(hidden_units))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(dropout))\n",
    "model.add(Dense(3))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(lr=lr_schedule(0))\n",
    "model.compile(loss='mse',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['mae'])\n",
    "\n",
    "# train the network\n",
    "H = model.fit(\n",
    "    feats_train_norm, targets_train,\n",
    "    validation_data=(feats_val_norm, targets_val), \n",
    "    batch_size=batch_size, \n",
    "    epochs=epochs,\n",
    "    shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take advantage of the code below to visualize the progression of your training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(np.arange(0, epochs), H.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(np.arange(0, epochs), H.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.title(\"MSE\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "save_fig(\"mse_loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(np.arange(0, epochs), H.history[\"mae\"], label=\"train_mae\")\n",
    "plt.plot(np.arange(0, epochs), H.history[\"val_mae\"], label=\"val_mae\")\n",
    "plt.title(\"MAE\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "save_fig(\"mae_loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to experiment with the network and let us know what result you got"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(feats_val_norm, batch_size=batch_size)\n",
    "mean_error_dist(targets_val, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feel free to experiment with the network or come up with a different DL approach. \n",
    "### The best mean error distance we got so far is 1.44.  Email us (<font color=blue>issai@nu.edu.kz</font>) the predictions of your best estimator on the test features AND your solutions to see how well you did !\n",
    "<font color=red> Please don't forget that the target values of test set are stacked in the order of csv files. If your test features do not follow this order, your result will be ruined.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(feats_test_norm, batch_size=batch_size)\n",
    "\n",
    "name = \"John\" # change to your first name\n",
    "surname = \"Snow\" # change to your lastname\n",
    "\n",
    "# email your csv file to issai@nu.edu.kz\n",
    "pd.DataFrame(preds).to_csv(\"{}_{}.csv\".format(name, surname), header=None, index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
